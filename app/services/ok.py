import hashlib
from typing import List, Dict
from selenium import webdriver
from selenium.webdriver import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import time
import re
import json
import os
import tempfile

from app import create_app
from app.database import db
from app.models.models import NewsSource, NewsPost
from app.services.texteditor import classify_text
from app.bot.core import save_bad_comment


class OKServiceSelenium:
	def __init__(self):
		self.driver = None
		self.cookies_file = "ok_cookies.json"
		# –°–æ–∑–¥–∞–µ–º Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
		self.app = create_app()
		self.setup_driver()

	def setup_driver(self):
		"""–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chrome WebDriver"""
		print("üîÑ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chrome WebDriver...")

		chrome_options = Options()
		chrome_options.add_argument("--no-sandbox")
		chrome_options.add_argument("--disable-dev-shm-usage")
		chrome_options.add_argument("--disable-gpu")
		chrome_options.add_argument("--window-size=1920,1080")
		chrome_options.add_argument("--disable-extensions")
		chrome_options.add_argument("--disable-notifications")

		chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
		chrome_options.add_experimental_option('useAutomationExtension', False)

		chrome_options.add_argument(
			"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

		try:
			service = Service(ChromeDriverManager().install())
			self.driver = webdriver.Chrome(service=service, options=chrome_options)
			self.driver.implicitly_wait(5)
			print("‚úÖ Chrome —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")

		except Exception as e:
			print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Chrome: {e}")
			raise

	def manual_login(self):
		"""–†—É—á–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"""
		print("=" * 60)
		print("–†–£–ß–ù–ê–Ø –ê–í–¢–û–†–ò–ó–ê–¶–ò–Ø")
		print("=" * 60)
		print("1. –ë—Ä–∞—É–∑–µ—Ä –æ—Ç–∫—Ä–æ–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤")
		print("2. –í–æ–π–¥–∏—Ç–µ –≤ —Å–≤–æ–π –∞–∫–∫–∞—É–Ω—Ç –≤—Ä—É—á–Ω—É—é")
		print("3. –ü–æ—Å–ª–µ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–∫—Ä–æ–π—Ç–µ –≤—Å–ø–ª—ã–≤–∞—é—â–∏–µ –æ–∫–Ω–∞")
		print("4. –ù–∞–∂–º–∏—Ç–µ Enter –≤ –∫–æ–Ω—Å–æ–ª–∏ —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")
		print("=" * 60)

		self.driver.get("https://ok.ru")
		time.sleep(3)

		input("–ü–æ—Å–ª–µ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞–∂–º–∏—Ç–µ Enter —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")
		self.save_cookies()
		return True

	def save_cookies(self):
		"""–°–æ—Ö—Ä–∞–Ω—è–µ–º cookies"""
		try:
			cookies = self.driver.get_cookies()
			with open(self.cookies_file, 'w', encoding='utf-8') as f:
				json.dump(cookies, f, ensure_ascii=False, indent=2)
			print("‚úÖ Cookies —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
		except Exception as e:
			print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies: {e}")

	def load_cookies(self):
		"""–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ cookies"""
		try:
			if os.path.exists(self.cookies_file):
				with open(self.cookies_file, 'r', encoding='utf-8') as f:
					cookies = json.load(f)

				self.driver.get("https://ok.ru")
				time.sleep(2)
				self.driver.delete_all_cookies()

				for cookie in cookies:
					try:
						self.driver.add_cookie(cookie)
					except:
						continue

				print("‚úÖ Cookies –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
				self.driver.refresh()
				time.sleep(3)
				return True
			return False
		except Exception as e:
			print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ cookies: {e}")
			return False

	def is_logged_in(self):
		"""–ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é"""
		try:
			self.driver.get("https://ok.ru")
			time.sleep(2)

			try:
				WebDriverWait(self.driver, 5).until(
					EC.presence_of_element_located((By.CSS_SELECTOR, ".ucard-mini, .nav-side_i, [data-l='t,userPage']"))
				)
				print("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞")
				return True
			except:
				current_url = self.driver.current_url
				if "login" in current_url or "auth" in current_url:
					return False
				return False

		except Exception as e:
			print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
			return False

	def ensure_logged_in(self):
		"""–£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã"""
		print("–ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é...")

		if self.load_cookies():
			if self.is_logged_in():
				print("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ cookies")
				return True

		print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")
		return self.manual_login()

	def _extract_comments_from_discussion_page(self, discussion_url, max_comments=50, wait_seconds=8):
		"""
		–û—Ç–∫—Ä—ã–≤–∞–µ—Ç discussion_url –≤ –Ω–æ–≤–æ–π –≤–∫–ª–∞–¥–∫–µ, –ø–∞—Ä—Å–∏—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç –≤–∫–ª–∞–¥–∫—É.
		"""
		orig_handle = self.driver.current_window_handle
		comments = []

		try:
			# –û—Ç–∫—Ä—ã—Ç—å –≤ –Ω–æ–≤–æ–π –≤–∫–ª–∞–¥–∫–µ
			self.driver.execute_script("window.open(arguments[0], '_blank');", discussion_url)
			time.sleep(2)

			# –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—É—é –≤–∫–ª–∞–¥–∫—É
			handles = self.driver.window_handles
			if len(handles) > 1:
				self.driver.switch_to.window(handles[-1])

				# –ñ–¥—ë–º –∑–∞–≥—Ä—É–∑–∫–∏
				time.sleep(3)

				# –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
				comment_selectors = [
					"div.comment_text",
					".comment-text",
					".comments_text",
					"[data-l='t,comment']",
					".textWrap",
					".d_comment_text"
				]

				for selector in comment_selectors:
					try:
						comment_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
						for el in comment_elements[:max_comments]:
							try:
								text = el.text.strip()
								if text and len(text) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
									sentiment = classify_text(text)
									comments.append({
										'text': text,
										'sentiment': sentiment,
										'classified_text': f"[{sentiment}] {text}"
									})
							except:
								continue
						if comments:
							break
					except:
						continue

		except Exception as e:
			print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏: {e}")

		finally:
			try:
				# –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤–∫–ª–∞–¥–∫—É –æ–±—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è
				if len(self.driver.window_handles) > 1:
					self.driver.close()
				self.driver.switch_to.window(orig_handle)
			except:
				pass

		# –û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–µ–π
		cleaned = []
		seen = set()
		for comment in comments:
			text = comment['text'].strip()
			if text and text not in seen:
				cleaned.append(comment)
				seen.add(text)

		print(f'–ù–∞–π–¥–µ–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(cleaned)}')
		return cleaned

	def save_post_to_db(self, post_data: dict, group_url: str):
		"""–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Å—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
		with self.app.app_context():
			try:
				# –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
				source = NewsSource.query.filter_by(
					platform='ok',
					source_id=group_url
				).first()

				if not source:
					source = NewsSource(
						platform='ok',
						source_id=group_url,
						source_name=group_url.split('/')[-1],  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è –≥—Ä—É–ø–ø—ã –∏–∑ URL
						source_type='group'
					)
					db.session.add(source)
					db.session.commit()  # –ö–æ–º–º–∏—Ç–∏–º —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å ID
					db.session.refresh(source)  # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç

				# –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –ø–æ—Å—Ç–∞
				unique_content = f"{post_data.get('text', '')}_{post_data.get('url', '')}"
				post_id_str = hashlib.md5(unique_content.encode()).hexdigest()

				# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –ø–æ—Å—Ç
				existing_post = NewsPost.query.filter_by(
					platform='ok',
					platform_post_id=post_id_str
				).first()

				if not existing_post:
					post = NewsPost(
						platform='ok',
						platform_post_id=post_id_str,
						source_id=source.id,
						text=post_data.get('text', ''),
						url=post_data.get('url', ''),
						author=source.source_name,
						publish_date=datetime.now(),  # –ú–æ–∂–Ω–æ –ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –∏–∑ –ø–æ—Å—Ç–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
						keywords=["ok", "—Å–æ—Ü—Å–µ—Ç—å"],
						platform_data={
							'comments_count': len(post_data.get('comments', [])),
							'source_group': group_url,
							'original_date': post_data.get('date', '')
						}
					)
					db.session.add(post)
					db.session.commit()
					db.session.refresh(post)
					print(f"‚úÖ –ü–æ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ë–î: {post.id}")
					return post
				else:
					print(f"‚ö†Ô∏è –ü–æ—Å—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {existing_post.id}")
					return existing_post

			except Exception as e:
				print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å—Ç–∞ –≤ –ë–î: {e}")
				import traceback
				traceback.print_exc()
				db.session.rollback()
				return None

	def save_bad_comments_to_json(self, post, comments):
		"""–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ JSON"""
		try:
			for comment in comments:
				if comment['sentiment'] == "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π":
					bad_comment_data = {
						'platform_comment_id': hashlib.md5(comment['text'].encode()).hexdigest(),
						'post_id': post.id,
						'post_title': post.text[:100] + '...' if post.text else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
						'post_url': post.url,
						'text': comment['text'],
						'user_id': 'anonymous',  # –í OK –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö
						'publish_date': datetime.now().isoformat(),
						'platform': 'OK',
						'sentiment': comment['sentiment'],
						'platform_data': {
							'source_group': post.platform_data.get('source_group', '')
						}
					}
					save_bad_comment(bad_comment_data)
		except Exception as e:
			print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–ª–æ—Ö–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}")

	def get_group_posts(self, group_url: str, count: int = 10, max_scrolls: int = 6, max_comments_per_post: int = 20):
		"""
		–°–æ–±–∏—Ä–∞–µ—Ç –ø–æ—Å—Ç—ã –∏–∑ –≥—Ä—É–ø–ø—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
		"""
		if not self.is_logged_in():
			print("‚ùå –ù–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã.")
			return []

		try:
			# –û—Ç–∫—Ä—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—É
			self.driver.get(group_url)
			# –ñ–¥—ë–º –ø–æ—è–≤–ª–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤
			WebDriverWait(self.driver, 12).until(
				EC.presence_of_element_located((By.CSS_SELECTOR, "div.feed_cnt, div.feed_b, div.media-block"))
			)

			# –°–∫—Ä–æ–ª–ª–∏–º –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏
			for i in range(max_scrolls):
				self.driver.execute_script("window.scrollBy(0, document.body.scrollHeight*0.6);")
				time.sleep(1.2)
				print(f"üìú –°–∫—Ä–æ–ª–ª {i + 1}/{max_scrolls}")

			posts_data = []
			seen_ids = set()

			# –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –ø–æ—Å—Ç–æ–≤
			post_containers = self.driver.find_elements(By.CSS_SELECTOR, "div.feed_cnt, div.feed_b, div.media-block")
			print(f"–ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –ø–æ—Å—Ç–æ–≤: {len(post_containers)}")

			for pc in post_containers[:count]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –ø–æ—Å—Ç–æ–≤
				try:
					# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
					text = ""
					try:
						text_elements = pc.find_elements(By.CSS_SELECTOR,
														 ".media-text_cnt_tx, .media-text_cnt, .ugc__text, .feed-content, .feed_text")
						for el in text_elements:
							if el.text.strip():
								text = el.text.strip()
								break
						if not text:
							text = pc.text.strip()[:600]
					except:
						text = pc.text.strip()[:600]

					# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –ø–æ—Å—Ç—ã
					if not text or len(text) < 10:
						continue

					# –°—Å—ã–ª–∫–∞
					url = ""
					try:
						a = pc.find_element(By.CSS_SELECTOR, "a.media-text_a, a[href*='/topic/'], a[href*='/story/']")
						href = a.get_attribute("href")
						if href:
							url = href if href.startswith("http") else "https://ok.ru" + href
					except:
						url = ""

					# –î–∞—Ç–∞
					date = ""
					try:
						time_el = pc.find_element(By.CSS_SELECTOR, "time")
						date = time_el.get_attribute("datetime") or time_el.text
					except:
						date = ""

					# –ü–∞—Ä—Å–∏–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
					comments = []
					try:
						comment_link = pc.find_element(By.CSS_SELECTOR,
													   "a[data-module='CommentWidgetsNew'], [data-module='CommentWidgetsNew']")
						href = comment_link.get_attribute("href") or ""
						if href:
							if href.startswith("/"):
								href = "https://ok.ru" + href
							comments = self._extract_comments_from_discussion_page(href,
																				   max_comments=max_comments_per_post)
					except Exception as e:
						# –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
						try:
							comment_links = pc.find_elements(By.CSS_SELECTOR,
															 "a[href*='comment'], a[href*='discussion']")
							for link in comment_links:
								href = link.get_attribute("href") or ""
								if href and ("comment" in href or "discussion" in href):
									if href.startswith("/"):
										href = "https://ok.ru" + href
									comments = self._extract_comments_from_discussion_page(href, max_comments=10)
									break
						except:
							pass

					# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
					filtered_comments = []
					STOP_WORDS = {"–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å", "–∫–æ–º–º–µ–Ω—Ç", "–∫–ª–∞—Å—Å", "–ª–∞–π–∫", "–ø–æ–¥–µ–ª–∏—Ç—å—Å—è", "–æ—Ç–≤–µ—Ç–∏—Ç—å"}
					for comment in comments:
						text = comment['text'].strip()
						if not text or len(text) < 5:
							continue
						if any(stop_word in text.lower() for stop_word in STOP_WORDS):
							continue
						filtered_comments.append(comment)

					post_data = {
						"text": text,
						"url": url,
						"date": date,
						"comments": filtered_comments
					}

					# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
					saved_post = self.save_post_to_db(post_data, group_url)
					if saved_post:
						# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ JSON
						self.save_bad_comments_to_json(saved_post, filtered_comments)
						posts_data.append(post_data)
						print(f"üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω –ø–æ—Å—Ç: {text[:50]}...")

				except Exception as ex:
					print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: {ex}")
					continue

			print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(posts_data)}")
			return posts_data

		except Exception as e:
			print(f"‚ùå –û—à–∏–±–∫–∞ –≤ get_group_posts: {e}")
			import traceback
			traceback.print_exc()
			return []

	def get_posts_from_groups(self,
							  group_urls: List[str],
							  per_group_count: int = 5,
							  total_limit: int = None,
							  delay_between_groups: float = 1.0,
							  max_scrolls: int = 6,
							  max_comments_per_post: int = 20,
							  tns_full_group_identifiers: List[str] = None,
							  keywords: List[str] = None) -> List[Dict]:

		if tns_full_group_identifiers is None:
			tns_full_group_identifiers = ["https://ok.ru/tnsenergon", "tnsenergon", "ok.ru/tnsenergon"]
		if keywords is None:
			keywords = ['–¢–ù–° —ç–Ω–µ—Ä–≥–æ –ù–ù', '–¢–ù–° —ç–Ω–µ—Ä–≥–æ –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥', '–¢–ù–°',
					  '–≠–Ω–µ—Ä–≥–æ—Å–±—ã—Ç –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥', "–¢–ù–° –ù–∏–∂–Ω–∏–π"]

		all_posts = []
		seen_ids = set()
		seen_urls = set()
		seen_hashes = set()

		def is_tns_full_group(gurl: str) -> bool:
			low = gurl.lower()
			for ident in tns_full_group_identifiers:
				if ident.lower() in low:
					return True
			return False

		def text_contains_keyword(text: str) -> bool:
			if not text:
				return False
			low = text.lower()
			for kw in keywords:
				if kw.lower() in low:
					return True
			return False

		for idx, gurl in enumerate(group_urls):
			if total_limit and len(all_posts) >= total_limit:
				break

			print(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã {idx + 1}/{len(group_urls)}: {gurl}")
			try:
				group_posts = self.get_group_posts(
					group_url=gurl,
					count=per_group_count,
					max_scrolls=max_scrolls,
					max_comments_per_post=max_comments_per_post
				)
			except Exception as e:
				print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ—Å—Ç–æ–≤ –≥—Ä—É–ø–ø—ã {gurl}: {e}")
				group_posts = []

			full_group = is_tns_full_group(gurl)
			group_posts_count = 0

			for p in group_posts:
				if total_limit and len(all_posts) >= total_limit:
					break

				post_id = p.get("id", "") or ""
				url = p.get("url", "") or ""
				text = p.get("text", "") or ""
				date = p.get("date", "") or ""

				# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç—ã –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –±–µ–∑ URL
				if not text and not url:
					continue

				# –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ "–ø–æ–ª–Ω–∞—è" –≥—Ä—É–ø–ø–∞ ‚Äî –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
				if not full_group:
					if not text_contains_keyword(text):
						continue

				# –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
				if post_id:
					key_type, key_val = "id", post_id
				elif url:
					key_type, key_val = "url", url
				else:
					key_type, key_val = "hash", hashlib.sha1(
						((text or "")[:300] + "|" + date).encode("utf-8")).hexdigest()

				is_dup = False
				if key_type == "id" and key_val in seen_ids:
					is_dup = True
				elif key_type == "url" and key_val in seen_urls:
					is_dup = True
				elif key_type == "hash" and key_val in seen_hashes:
					is_dup = True

				if is_dup:
					continue

				record = {
					"source_group": gurl,
					"id": post_id,
					"date": date,
					"text": text,
					"comments": [c['classified_text'] for c in p.get("comments", [])],
					"url": url
				}
				all_posts.append(record)
				group_posts_count += 1

				if key_type == "id":
					seen_ids.add(key_val)
				elif key_type == "url":
					seen_urls.add(key_val)
				else:
					seen_hashes.add(key_val)

			print(f"‚úÖ –ò–∑ –≥—Ä—É–ø–ø—ã {gurl} –¥–æ–±–∞–≤–ª–µ–Ω–æ {group_posts_count} –ø–æ—Å—Ç–æ–≤")
			time.sleep(delay_between_groups)

		print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –ø–æ—Å—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö –≥—Ä—É–ø–ø: {len(all_posts)}")
		return all_posts

	def close(self):
		"""–ó–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞"""
		if self.driver:
			self.driver.quit()
			print("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")


if __name__ == "__main__":
	groups = [
		"https://ok.ru/group/70000037422012",
		"https://ok.ru/tnsenergon",
		"https://ok.ru/kpnnovgorod",
		"https://ok.ru/newsnnru",
		"https://ok.ru/nizhegorodnosti"
	]

	service = OKServiceSelenium()
	try:
		if not service.ensure_logged_in():
			print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è")
		else:
			posts = service.get_posts_from_groups(
				groups,
				per_group_count=5,
				total_limit=20,
				delay_between_groups=1.5,
				tns_full_group_identifiers=["https://ok.ru/tnsenergon", "tnsenergon"],
				keywords=['–¢–ù–° —ç–Ω–µ—Ä–≥–æ –ù–ù', '–¢–ù–° —ç–Ω–µ—Ä–≥–æ –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥', '–¢–ù–°',
					  '–≠–Ω–µ—Ä–≥–æ—Å–±—ã—Ç –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥', "–¢–ù–° –ù–∏–∂–Ω–∏–π"]
			)

			for i, p in enumerate(posts, 1):
				print(f"{i}. {p['source_group']} - {p['url']} - {(p['text'] or '')[:120]}")
				if p['comments']:
					print(f"   –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏: {len(p['comments'])}")
	finally:
		service.close()