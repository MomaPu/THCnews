from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import time
import re
import asyncio
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse


class BrowserServiceSelenium:
	def __init__(self):
		self.driver = None
		self.setup_driver()

	def setup_driver(self):
		"""–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chrome WebDriver —Å webdriver-manager"""
		chrome_options = Options()
		chrome_options.add_argument("--disable-gpu")
		chrome_options.add_argument("--no-sandbox")
		chrome_options.add_argument("--disable-dev-shm-usage")
		chrome_options.add_argument("--disable-blink-features=AutomationControlled")
		chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
		chrome_options.add_experimental_option('useAutomationExtension', False)
		chrome_options.add_argument("--window-size=1920,1080")
		chrome_options.add_argument("--headless")  # –î–æ–±–∞–≤–ª—è–µ–º headless —Ä–µ–∂–∏–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏

		chrome_options.add_argument(
			"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

		service = Service(ChromeDriverManager().install())
		self.driver = webdriver.Chrome(service=service, options=chrome_options)
		self.driver.implicitly_wait(5)  # –£–º–µ–Ω—å—à–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è

	def get_posts_from_site(self, url):
		"""–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
		print(f"–ü–∞—Ä—Å–∏–º —Å–∞–π—Ç: {url}")

		# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–∞–π—Ç–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä—Å–µ—Ä
		domain = urlparse(url).netloc

		if 'kp.ru' in domain:
			return self._parse_kp_ru(url)
		elif 'aif.ru' in domain:
			return self._parse_aif_ru(url)
		elif 'mk.ru' in domain:
			return self._parse_mk_ru(url)
		elif 'dzen.ru' in domain:
			return self._parse_dzen(url)
		else:
			return self._parse_generic_site(url)


	def _parse_kp_ru(self, url):
		"""–ü–∞—Ä—Å–µ—Ä –¥–ª—è kp.ru"""
		print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è kp.ru")
		self.driver.get(url)
		time.sleep(3)

		# –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è kp.ru
		selectors = [
			"article",
			".sc-1tputnk-3",  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–ª–∞—Å—Å—ã kp.ru
			".sc-1tputnk-8",
			".sc-1tputnk-2",
			"[data-l='t,mediaTopic']",
			".media-topic",
			".feed_w",
			".post",
			".topic"
		]

		return self._find_and_parse_elements(selectors)

	def _parse_aif_ru(self, url):
		"""–ü–∞—Ä—Å–µ—Ä –¥–ª—è aif.ru"""
		print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è aif.ru")
		self.driver.get(url)
		time.sleep(3)

		# –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è aif.ru
		selectors = [
			"article",
			".article",
			".news_item",
			".b-article",
			".item",
			"[data-type='article']",
			".js-article-item",
			".rubric_news_list_item"
		]

		return self._find_and_parse_elements(selectors)

	def _parse_mk_ru(self, url):
		"""–ü–∞—Ä—Å–µ—Ä –¥–ª—è mk.ru"""
		print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è mk.ru")
		self.driver.get(url)
		time.sleep(3)

		# –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è mk.ru
		selectors = [
			".news-listing__item",
			".article",
			".news-item",
			".listing-item",
			".item",
			"[data-article]",
			"article",
			".news-block"
		]

		return self._find_and_parse_elements(selectors)

	def _parse_generic_site(self, url):
		"""–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤"""
		print("–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä")
		self.driver.get(url)
		time.sleep(3)

		# –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
		selectors = [
			"article",
			".article",
			".news-item",
			".post",
			".item",
			".card",
			"[data-id]",
			".story",
			".news"
		]

		return self._find_and_parse_elements(selectors)

	def _parse_dzen(self, url):
		"""–ü–∞—Ä—Å–µ—Ä –¥–ª—è dzen"""
		print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è dzen")
		self.driver.get(url)
		time.sleep(4)

		# –ë–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è Dzen - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
		selectors = [
			# –°–∞–º—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã first
			"[data-testid='news-item']",
			"[data-testid='news-card']",
			"[data-testid='card-news']",
			".news-card",
			".news-item",
			"[role='article']",
			"article"
		]

		# –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å–∞–º—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
		posts = self._find_and_parse_elements(selectors)

		# –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Å—Ç–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
		if len(posts) >= 10:
			print(f"–ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã")
			return posts

		# –ï—Å–ª–∏ –º–∞–ª–æ –ø–æ—Å—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –±–æ–ª–µ–µ –æ–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
		print("–ü—Ä–æ–±—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º...")

		additional_selectors = [
			"[data-testid*='news']",
			"[data-testid*='card']",
			"[class*='news-card']",
			"[class*='news-item']"
		]

		for selector in additional_selectors:
			try:
				elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
				if elements:
					print(f"–ù–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å: {selector}")
					# –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 20 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å
					limited_elements = elements[:20]
					text_elements = [el for el in limited_elements if el.text.strip() and len(el.text.strip()) > 30]

					for element in text_elements:
						try:
							parsed_post = self._parse_post_element(element)
							if parsed_post and parsed_post['text']:
								posts.append(parsed_post)
						except:
							continue

					if len(posts) >= 15:  # –ï—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Å—Ç–æ–≤
						break

			except Exception as e:
				continue

		print(f"–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—à–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ —Å Dzen")
		return posts

	def _find_and_parse_elements(self, selectors):
		"""–ü–æ–∏—Å–∫ –∏ –ø–∞—Ä—Å–∏–Ω–≥ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º"""
		posts = []

		# –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
		self.driver.execute_script("window.scrollTo(0, 500);")
		time.sleep(1)

		for selector in selectors:
			try:
				elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
				if elements:
					print(f"–ù–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector}")
					# –ë–æ–ª–µ–µ –ª–∏–±–µ—Ä–∞–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
					text_elements = [el for el in elements if el.text.strip() and len(el.text.strip()) > 30]
					if text_elements:
						posts.extend(text_elements)
			except Exception as e:
				continue

		# –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ —Ç–µ–∫—Å—Ç—É
		if len(posts) < 5:
			try:
				all_elements = self.driver.find_elements(By.XPATH, "//*[string-length(text()) > 50]")
				posts.extend(all_elements[:10])
			except:
				pass

		# –ü–∞—Ä—Å–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã
		parsed_posts = []
		for post_element in posts:
			try:
				parsed_post = self._parse_post_element(post_element)
				if parsed_post and parsed_post['text']:
					parsed_posts.append(parsed_post)
			except Exception as e:
				continue

		print(f"–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—à–µ–Ω–æ {len(parsed_posts)} –ø–æ—Å—Ç–æ–≤")
		return parsed_posts

	def _parse_post_element(self, post_element):
		try:
			text = post_element.text.strip()

			if not text or len(text) < 30:
				return None

			# –ò—â–µ–º –¥–∞—Ç—É
			date_patterns = [
				r'\d{1,2}\s+[–∞-—è]+\s+\d{4}',
				r'\d{1,2}\.\d{1,2}\.\d{4}',
				r'\d{1,2}:\d{2}',
				r'–≤—á–µ—Ä–∞',
				r'—Å–µ–≥–æ–¥–Ω—è',
				r'\d+\s+–º–∏–Ω—É—Ç',
				r'\d+\s+—á–∞—Å',
			]

			date_str = ""
			for pattern in date_patterns:
				match = re.search(pattern, text, re.IGNORECASE)
				if match:
					date_str = match.group()
					break

			# –ò—â–µ–º —Å—Å—ã–ª–∫—É
			link = ""
			try:
				# –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –∏ –µ–≥–æ —Ä–æ–¥–∏—Ç–µ–ª—è—Ö
				current = post_element
				for _ in range(3):
					try:
						links = current.find_elements(By.TAG_NAME, "a")
						for link_element in links:
							href = link_element.get_attribute("href")
							if href and href.startswith('http'):
								link = href
								break
						if link:
							break
					except:
						pass
					current = current.find_element(By.XPATH, "./..")
			except:
				pass

			return {
				'text': text,
				'date': date_str or datetime.now().strftime('%Y-%m-%d %H:%M'),
				'url': link or self.driver.current_url,
				'source': self.driver.current_url
			}

		except Exception as e:
			return None

	def search_posts_with_keyword(self, url, keyword, count=5):
		"""–ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º"""
		print(f"–ò—â–µ–º –ø–æ—Å—Ç—ã —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º: '{keyword}'")
		posts = self.get_posts_from_site(url)

		if not posts:
			print("–ü–æ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
			return []

		found_posts = []
		for post in posts:
			post_text = post.get('text', '').lower()
			if keyword.lower() in post_text:
				found_posts.append(post)

		print(f"–ù–∞–π–¥–µ–Ω–æ {len(found_posts)} –ø–æ—Å—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º '{keyword}'")
		return found_posts[:count]

	def get_posts(self, url, keyword, count=3):
		"""–ê–ª–∏–∞—Å –¥–ª—è search_posts_with_keyword"""
		return self.search_posts_with_keyword(url, keyword, count)

	def close(self):
		"""–ó–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞"""
		if self.driver:
			self.driver.quit()


class AsyncNewsParser:
	"""–ö–ª–∞—Å—Å –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∞–π—Ç–æ–≤"""

	def __init__(self):
		self.executor = ThreadPoolExecutor(max_workers=3)

	async def parse_site_async(self, url, keyword, count=3):
		"""–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞"""
		loop = asyncio.get_event_loop()

		try:
			result = await loop.run_in_executor(
				self.executor,
				self._parse_site_sync,
				url, keyword, count
			)
			return result
		except Exception as e:
			print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
			return {'url': url, 'posts': [], 'count': 0}

	def _parse_site_sync(self, url, keyword, count):
		"""–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
		service = BrowserServiceSelenium()
		try:
			posts = service.search_posts_with_keyword(url, keyword, count)
			return {
				'url': url,
				'posts': posts,
				'count': len(posts)
			}
		except Exception as e:
			print(f"–û—à–∏–±–∫–∞ –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
			return {'url': url, 'posts': [], 'count': 0}
		finally:
			service.close()

	async def parse_multiple_sites(self, sites_keywords, count=3):
		"""–ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∞–π—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"""
		tasks = []

		for site_info in sites_keywords:
			url = site_info['url']
			keyword = site_info['keyword']
			task = self.parse_site_async(url, keyword, count)
			tasks.append(task)

		results = await asyncio.gather(*tasks, return_exceptions=True)

		valid_results = []
		for result in results:
			if isinstance(result, dict) and 'posts' in result:
				valid_results.append(result)

		return valid_results

	def close(self):
		"""–ó–∞–∫—Ä—ã—Ç–∏–µ executor"""
		self.executor.shutdown()


async def async_parse_news_sites():
	"""–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
	parser = AsyncNewsParser()

	# –°–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
	sites_to_parse = [
		{'url': 'https://www.nnov.kp.ru/online/', 'keyword': '–ü—É—Ç–∏–Ω'},
		{'url': 'https://nn.aif.ru/', 'keyword': '–ü—É—Ç–∏–Ω'},
		{'url': 'https://nn.mk.ru/', 'keyword': '–ü—É—Ç–∏–Ω'},
		{'url': 'https://dzen.ru/news/region/nizhny_novgorod', 'keyword': '–í–ª–∞–¥–∏–º–∏—Ä'}
	]

	try:
		print("=" * 60)
		print("üîÑ –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –ù–û–í–û–°–¢–ù–´–• –°–ê–ô–¢–û–í")
		print("=" * 60)

		start_time = time.time()
		results = await parser.parse_multiple_sites(sites_to_parse, count=4)
		end_time = time.time()

		print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
		print("\n" + "=" * 60)
		print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–°–ò–ù–•–†–û–ù–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê:")
		print("=" * 60)

		total_posts = 0
		print("\n" + "=" * 60)
		print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–°–ò–ù–•–†–û–ù–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê:")
		print("=" * 60)

		total_posts = 0
		for result in results:
			print(f"\nüåê –°–∞–π—Ç: {result['url']}")
			print(f"üìà –ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {result['count']}")

			if result['count'] > 0:
				for i, post in enumerate(result['posts'][:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø–æ—Å—Ç–∞
					print(f"\n   üìù –ü–æ—Å—Ç {i}:")
					print(f"   üìÖ –î–∞—Ç–∞: {post['date']}")
					print(f"   üîó –°—Å—ã–ª–∫–∞: {post['url'][:70]}...")
					print(f"   üìÑ –¢–µ–∫—Å—Ç: {post['text'][:120]}...")
					print("   " + "-" * 40)
			else:
				print("   ‚ùå –ü–æ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

			total_posts += result['count']

		print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –Ω–∞ –≤—Å–µ—Ö —Å–∞–π—Ç–∞—Ö: {total_posts}")
	finally:
		parser.close()


def sync_parse_single_site(url, keyword="–Ω–æ–≤–æ—Å—Ç–∏", count=3):
	"""–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞"""
	service = BrowserServiceSelenium()

	try:
		print("=" * 60)
		print(f"üîÑ –ü–ê–†–°–ò–ù–ì –°–ê–ô–¢–ê: {url}")
		print("=" * 60)

		results = service.search_posts_with_keyword(url, keyword, count)

		print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {url}:")
		for i, post in enumerate(results, 1):
			print(f"\nüìù –ü–æ—Å—Ç {i}:")
			print(f"üìÖ –î–∞—Ç–∞: {post['date']}")
			print(f"üîó –°—Å—ã–ª–∫–∞: {post['url'][:50]}...")
			print(f"üìÑ –¢–µ–∫—Å—Ç: {post['text'][:100]}...")

	finally:
		service.close()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
	# –í–∞—Ä–∏–∞–Ω—Ç 1: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
	asyncio.run(async_parse_news_sites())

	print("\n" + "=" * 80)

