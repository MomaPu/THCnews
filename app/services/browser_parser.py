import hashlib
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import time
import re
from urllib.parse import urlparse
from typing import List, Dict, Optional
from app import create_app
from app.database import db
from app.models.models import NewsSource, NewsPost


class WebParserSelenium:
    def __init__(self):
        self.driver = None
        # –°–æ–∑–¥–∞–µ–º Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
        self.app = create_app()
        self.setup_driver()
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.keywords = ['–¢–ù–° —ç–Ω–µ—Ä–≥–æ –ù–ù', '–¢–ù–° —ç–Ω–µ—Ä–≥–æ –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥', '–¢–ù–°',
					  '–≠–Ω–µ—Ä–≥–æ—Å–±—ã—Ç –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥', "–¢–ù–° –ù–∏–∂–Ω–∏–π"]

    def setup_driver(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Chrome WebDriver"""
        chrome_options = Options()
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--headless")

        chrome_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.implicitly_wait(5)
            print("‚úÖ –í–µ–±-–ø–∞—Ä—Å–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–ø–∞—Ä—Å–µ—Ä–∞: {e}")
            raise

    def _contains_keywords(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.keywords)

    def save_post_to_db(self, post_data: dict, source_url: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Å—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        with self.app.app_context():
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø–æ—Å—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                if not self._contains_keywords(post_data.get('text', '')):
                    return False

                # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                source = NewsSource.query.filter_by(
                    platform='web',
                    source_id=source_url
                ).first()

                if not source:
                    source_name = urlparse(source_url).netloc
                    source = NewsSource(
                        platform='web',
                        source_id=source_url,
                        source_name=source_name,
                        source_type='website'
                    )
                    db.session.add(source)
                    db.session.flush()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –ø–æ—Å—Ç
                post_hash = hashlib.md5(post_data['text'].encode()).hexdigest()
                existing_post = NewsPost.query.filter_by(
                    platform='web',
                    platform_post_id=post_hash
                ).first()

                if not existing_post:
                    post = NewsPost(
                        platform='web',
                        platform_post_id=post_hash,
                        source_id=source.id,
                        text=post_data.get('text', ''),
                        url=post_data.get('url', ''),
                        author=source.source_name,
                        publish_date=datetime.now(),
                        platform_data={
                            'source': source_url,
                            'date_str': post_data.get('date', ''),
                            'original_source': post_data.get('source', '')
                        }
                    )
                    db.session.add(post)
                    db.session.commit()
                    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ø–æ—Å—Ç —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏: {post_data.get('text', '')[:50]}...")
                    return True
                return False

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å—Ç–∞ –≤ –ë–î: {e}")
                db.session.rollback()
                return False

    def get_posts_data(self, url: str, count: int = 5) -> List[Dict]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ –ë–î"""
        print(f"üåê –ü–∞—Ä—Å–∏–º —Å–∞–π—Ç: {url}")
        print(f"üîç –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(self.keywords)}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–∞–π—Ç–∞
        domain = urlparse(url).netloc

        if 'kp.ru' in domain:
            posts = self._parse_kp_ru(url, count)
        elif 'aif.ru' in domain:
            posts = self._parse_aif_ru(url, count)
        elif 'mk.ru' in domain:
            posts = self._parse_mk_ru(url, count)
        elif 'dzen.ru' in domain:
            posts = self._parse_dzen(url, count)
        else:
            posts = self._parse_generic_site(url, count)

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        filtered_posts = []
        for post in posts:
            if self._contains_keywords(post.get('text', '')):
                filtered_posts.append(post)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –≤ –ë–î
        saved_posts = []
        for post in filtered_posts:
            if self.save_post_to_db(post, url):
                saved_posts.append(post)

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(filtered_posts)} –ø–æ—Å—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏")
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î: {len(saved_posts)} –ø–æ—Å—Ç–æ–≤")
        return saved_posts

    def _parse_kp_ru(self, url: str, count: int) -> List[Dict]:
        """–ü–∞—Ä—Å–µ—Ä –¥–ª—è kp.ru"""
        print("üì∞ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è kp.ru")
        self.driver.get(url)
        time.sleep(3)

        selectors = [
            "article", ".sc-1tputnk-3", ".sc-1tputnk-8",
            ".sc-1tputnk-2", "[data-l='t,mediaTopic']",
            ".media-topic", ".feed_w", ".post", ".topic"
        ]

        return self._find_and_parse_elements(selectors, count, url)

    def _parse_aif_ru(self, url: str, count: int) -> List[Dict]:
        """–ü–∞—Ä—Å–µ—Ä –¥–ª—è aif.ru"""
        print("üì∞ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è aif.ru")
        self.driver.get(url)
        time.sleep(3)

        selectors = [
            "article", ".article", ".news_item", ".b-article",
            ".item", "[data-type='article']", ".js-article-item",
            ".rubric_news_list_item"
        ]

        return self._find_and_parse_elements(selectors, count, url)

    def _parse_mk_ru(self, url: str, count: int) -> List[Dict]:
        """–ü–∞—Ä—Å–µ—Ä –¥–ª—è mk.ru"""
        print("üì∞ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è mk.ru")
        self.driver.get(url)
        time.sleep(3)

        selectors = [
            ".news-listing__item", ".article", ".news-item",
            ".listing-item", ".item", "[data-article]", "article", ".news-block"
        ]

        return self._find_and_parse_elements(selectors, count, url)

    def _parse_dzen(self, url: str, count: int) -> List[Dict]:
        """–ü–∞—Ä—Å–µ—Ä –¥–ª—è dzen"""
        print("üì∞ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –¥–ª—è dzen")
        self.driver.get(url)
        time.sleep(4)

        selectors = [
            "[data-testid='news-item']", "[data-testid='news-card']",
            "[data-testid='card-news']", ".news-card",
            ".news-item", "[role='article']", "article"
        ]

        posts = self._find_and_parse_elements(selectors, count, url)

        if len(posts) >= count:
            return posts

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        additional_selectors = [
            "[data-testid*='news']", "[data-testid*='card']",
            "[class*='news-card']", "[class*='news-item']"
        ]

        for selector in additional_selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements[:10]:
                    post_data = self._parse_post_element(element, url)
                    if post_data:
                        posts.append(post_data)
                        if len(posts) >= count:
                            break
                if len(posts) >= count:
                    break
            except:
                continue

        return posts[:count]

    def _parse_generic_site(self, url: str, count: int) -> List[Dict]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å–∞–π—Ç–æ–≤"""
        print("üì∞ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä")
        self.driver.get(url)
        time.sleep(3)

        selectors = [
            "article", ".article", ".news-item", ".post",
            ".item", ".card", "[data-id]", ".story", ".news"
        ]

        return self._find_and_parse_elements(selectors, count, url)

    def _find_and_parse_elements(self, selectors: List[str], count: int, source_url: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∏ –ø–∞—Ä—Å–∏–Ω–≥ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º"""
        posts = []
        try:
            self.driver.execute_script("window.scrollTo(0, 500);")
            time.sleep(1)

            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        post_data = self._parse_post_element(element, source_url)
                        if post_data:
                            posts.append(post_data)
                            if len(posts) >= count:
                                break
                    if len(posts) >= count:
                        break
                except:
                    continue
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")

        return posts[:count]

    def _parse_post_element(self, post_element, source_url: str) -> Optional[Dict]:
        try:
            text = post_element.text.strip()
            if not text or len(text) < 50:
                return None

            # –ò—â–µ–º –¥–∞—Ç—É
            date_str = ""
            date_patterns = [
                r'\d{1,2}\s+[–∞-—è]+\s+\d{4}', r'\d{1,2}\.\d{1,2}\.\d{4}',
                r'\d{1,2}:\d{2}', r'–≤—á–µ—Ä–∞', r'—Å–µ–≥–æ–¥–Ω—è',
                r'\d+\s+–º–∏–Ω—É—Ç', r'\d+\s+—á–∞—Å',
            ]

            for pattern in date_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    date_str = match.group()
                    break

            # –ò—â–µ–º —Å—Å—ã–ª–∫—É
            link = source_url
            try:
                links = post_element.find_elements(By.TAG_NAME, "a")
                for link_element in links:
                    href = link_element.get_attribute("href")
                    if href and href.startswith('http'):
                        link = href
                        break
            except:
                pass

            return {
                'text': text,
                'date': date_str,
                'url': link,
                'source': source_url
            }

        except Exception as e:
            return None

    def search_posts_with_keyword(self, url, keyword, count=5):
        """–ü–æ–∏—Å–∫ –ø–æ—Å—Ç–æ–≤ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º"""
        print(f"–ò—â–µ–º –ø–æ—Å—Ç—ã —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º: '{keyword}'")
        posts = self.get_posts_data(url)

        if not posts:
            print("–ü–æ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return []

        found_posts = []
        for post in posts:
            post_text = post.get('text', '').lower()
            if keyword.lower() in post_text:
                found_posts.append(post)

        print(f"–ù–∞–π–¥–µ–Ω–æ {len(found_posts)} –ø–æ—Å—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º '{keyword}'")
        return found_posts[:count]

    def parse_multiple_sites(self, urls: List[str], posts_per_site: int = 3) -> Dict[str, List[Dict]]:
        """–ü–∞—Ä—Å–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∞–π—Ç–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        results = {}

        for url in urls:
            try:
                print(f"\nüîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {url}")
                posts = self.get_posts_data(url, posts_per_site)
                results[url] = posts
                print(f"‚úÖ –°–∞–π—Ç {url}: {len(posts)} –ø–æ—Å—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
                results[url] = []

        return results

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if self.driver:
            self.driver.quit()
            print("‚úÖ –í–µ–±-–ø–∞—Ä—Å–µ—Ä –∑–∞–∫—Ä—ã—Ç")


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Flask
def parse_news_sites():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
    parser = WebParserSelenium()
    try:
        sites = [
            "https://www.nnov.kp.ru/online/",
            "https://nn.aif.ru/",
            "https://dzen.ru/news/region/nizhny_novgorod",
            "https://www.mk.ru/",
            "https://ria.ru/",
            "https://www.rbc.ru/",
            "https://www.kommersant.ru/",
            "https://lenta.ru/",
            "https://news.mail.ru/",
            "https://news.yandex.ru/region/nizhny_novgorod"
        ]

        results = parser.parse_multiple_sites(sites, posts_per_site=3)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_posts = 0
        for url, posts in results.items():
            print(f"üìä {url}: {len(posts)} –ø–æ—Å—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏")
            total_posts += len(posts)

        print(f"\nüéØ –í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏: {total_posts}")
        return results

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return {}
    finally:
        parser.close()


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–±-–ø–∞—Ä—Å–µ—Ä–∞...")
    print("üîç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –¢–ù–° –≠–Ω–µ—Ä–≥–æ, —Ç–Ω—Å, —ç–Ω–µ—Ä–≥–æ, —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ")
    results = parse_news_sites()

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    for url, posts in results.items():
        print(f"\nüåê {url}:")
        for i, post in enumerate(posts, 1):
            print(f"  {i}. {post.get('text', '')[:100]}...")